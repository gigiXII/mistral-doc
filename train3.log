The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
[2025-06-20 21:47:53,412] [INFO] [datasets.<module>:54] [PID:3666] PyTorch version 2.5.1+cu124 available.
[2025-06-20 21:47:54,484] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-20 21:47:54,769] [INFO] [root.spawn:60] [PID:3666] gcc -pthread -B /root/miniconda3/envs/py3.11/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /root/miniconda3/envs/py3.11/include -fPIC -O2 -isystem /root/miniconda3/envs/py3.11/include -fPIC -c /tmp/tmpgr9wg2tv/test.c -o /tmp/tmpgr9wg2tv/test.o
[2025-06-20 21:47:54,801] [INFO] [root.spawn:60] [PID:3666] gcc -pthread -B /root/miniconda3/envs/py3.11/compiler_compat /tmp/tmpgr9wg2tv/test.o -laio -o /tmp/tmpgr9wg2tv/a.out
[2025-06-20 21:47:55,275] [INFO] [root.spawn:60] [PID:3666] gcc -pthread -B /root/miniconda3/envs/py3.11/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /root/miniconda3/envs/py3.11/include -fPIC -O2 -isystem /root/miniconda3/envs/py3.11/include -fPIC -c /tmp/tmpdazu2u79/test.c -o /tmp/tmpdazu2u79/test.o
[2025-06-20 21:47:55,307] [INFO] [root.spawn:60] [PID:3666] gcc -pthread -B /root/miniconda3/envs/py3.11/compiler_compat /tmp/tmpdazu2u79/test.o -L/usr/local/cuda -L/usr/local/cuda/lib64 -lcufile -o /tmp/tmpdazu2u79/a.out
/workspace/axolotl/src/axolotl/monkeypatch/relora.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import ZeroRedundancyOptimizer
[33m[2025-06-20 21:47:55,998] [WARNING] [axolotl.utils.config.models.input.hint_lora_8bit:1425] [PID:3666] [RANK:0] We recommend setting `load_in_8bit: true` for LORA finetuning[39m
[2025-06-20 21:47:56,516] [INFO] [axolotl.normalize_config:236] [PID:3666] [RANK:0] cuda memory usage baseline: 0.000GB (+0.370GB misc)[39m

     #@@ #@@      @@# @@#
    @@  @@          @@  @@           =@@#                               @@                 #@    =@@#.
    @@    #@@@@@@@@@    @@           #@#@=                              @@                 #@     .=@@
      #@@@@@@@@@@@@@@@@@            =@# @#     ##=     ##    =####=+    @@      =#####+  =#@@###.   @@
    @@@@@@@@@@/  +@@/  +@@          #@  =@=     #@=   @@   =@#+  +#@#   @@    =@#+  +#@#   #@.      @@
    @@@@@@@@@@  ##@@  ##@@         =@#   @#      =@# @#    @@      @@   @@    @@      #@   #@       @@
     @@@@@@@@@@@@@@@@@@@@          #@=+++#@=      =@@#     @@      @@   @@    @@      #@   #@       @@
                                  =@#=====@@     =@# @#    @@      @@   @@    @@      #@   #@       @@
    @@@@@@@@@@@@@@@@  @@@@        #@      #@=   #@=  +@@   #@#    =@#   @@.   =@#    =@#   #@.      @@
                                 =@#       @#  #@=     #@   =#@@@@#=    +#@@=  +#@@@@#=    .##@@+   @@
    @@@@  @@@@@@@@@@@@@@@@

[2025-06-20 21:47:59,753] [DEBUG] [axolotl.load_tokenizer:401] [PID:3666] [RANK:0] EOS: 2 / </s>[39m
[2025-06-20 21:47:59,753] [DEBUG] [axolotl.load_tokenizer:402] [PID:3666] [RANK:0] BOS: 1 / <s>[39m
[2025-06-20 21:47:59,753] [DEBUG] [axolotl.load_tokenizer:403] [PID:3666] [RANK:0] PAD: 2 / </s>[39m
[2025-06-20 21:47:59,753] [DEBUG] [axolotl.load_tokenizer:404] [PID:3666] [RANK:0] UNK: 0 / <unk>[39m
[2025-06-20 21:47:59,754] [INFO] [axolotl.utils.data.sft.load_tokenized_prepared_datasets:252] [PID:3666] [RANK:0] Unable to find prepared dataset in last_run_prepared/c0dc037b6c74c09d179d6681f607b1e5[39m
[2025-06-20 21:47:59,754] [INFO] [axolotl.utils.data.sft.load_tokenized_prepared_datasets:253] [PID:3666] [RANK:0] Loading raw datasets...[39m
[33m[2025-06-20 21:47:59,754] [WARNING] [axolotl.utils.data.sft.load_tokenized_prepared_datasets:255] [PID:3666] [RANK:0] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset.[39m
[2025-06-20 21:47:59,754] [INFO] [axolotl.utils.data.sft.load_tokenized_prepared_datasets:262] [PID:3666] [RANK:0] No seed provided, using default seed of 42[39m
Generating train split:   0%|          | 0/179 [00:00<?, ? examples/s]Generating train split: 100%|██████████| 179/179 [00:00<00:00, 1439.83 examples/s]Generating train split: 100%|██████████| 179/179 [00:00<00:00, 1041.14 examples/s]
Generating test split:   0%|          | 0/20 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 20/20 [00:00<00:00, 675.07 examples/s]
[2025-06-20 21:48:06,711] [INFO] [axolotl.utils.data.sft.get_dataset_wrapper:457] [PID:3666] [RANK:0] Loading dataset with base_type: completion and prompt_style: None[39m
Tokenizing Prompts (num_proc=32):   0%|          | 0/179 [00:00<?, ? examples/s]Tokenizing Prompts (num_proc=32):   3%|▎         | 6/179 [00:00<00:05, 33.51 examples/s]Tokenizing Prompts (num_proc=32):  13%|█▎        | 23/179 [00:00<00:02, 60.93 examples/s]Tokenizing Prompts (num_proc=32):  30%|██▉       | 53/179 [00:00<00:00, 129.91 examples/s]Tokenizing Prompts (num_proc=32):  45%|████▍     | 80/179 [00:00<00:00, 160.41 examples/s]Tokenizing Prompts (num_proc=32):  56%|█████▋    | 101/179 [00:00<00:00, 134.22 examples/s]Tokenizing Prompts (num_proc=32):  72%|███████▏  | 129/179 [00:00<00:00, 165.58 examples/s]Tokenizing Prompts (num_proc=32):  84%|████████▍ | 151/179 [00:01<00:00, 174.34 examples/s]Tokenizing Prompts (num_proc=32):  97%|█████████▋| 174/179 [00:01<00:00, 88.70 examples/s] Tokenizing Prompts (num_proc=32): 100%|██████████| 179/179 [00:01<00:00, 97.77 examples/s]
[2025-06-20 21:48:09,439] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:177] [PID:3666] [RANK:0] min_input_len: 6[39m
[2025-06-20 21:48:09,439] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:179] [PID:3666] [RANK:0] max_input_len: 2048[39m
Dropping Long Sequences (num_proc=32):   0%|          | 0/2107 [00:00<?, ? examples/s]Dropping Long Sequences (num_proc=32):   3%|▎         | 66/2107 [00:00<00:05, 353.51 examples/s]Dropping Long Sequences (num_proc=32):  25%|██▌       | 527/2107 [00:00<00:00, 2173.44 examples/s]Dropping Long Sequences (num_proc=32): 100%|██████████| 2107/2107 [00:00<00:00, 4368.16 examples/s]
[2025-06-20 21:48:10,892] [INFO] [axolotl.utils.data.sft.load_tokenized_prepared_datasets:332] [PID:3666] [RANK:0] Saving merged prepared dataset to disk... last_run_prepared/c0dc037b6c74c09d179d6681f607b1e5[39m
Saving the dataset (0/1 shards):   0%|          | 0/2107 [00:00<?, ? examples/s]Saving the dataset (1/1 shards): 100%|██████████| 2107/2107 [00:00<00:00, 27278.03 examples/s]Saving the dataset (1/1 shards): 100%|██████████| 2107/2107 [00:00<00:00, 27184.05 examples/s]
[2025-06-20 21:48:11,196] [DEBUG] [axolotl.calculate_total_num_steps:403] [PID:3666] [RANK:0] total_num_tokens: 3_917_447[39m
[2025-06-20 21:48:11,240] [DEBUG] [axolotl.calculate_total_num_steps:421] [PID:3666] [RANK:0] `total_supervised_tokens: 4_127_813`[39m
[2025-06-20 21:48:11,240] [DEBUG] [axolotl.calculate_total_num_steps:499] [PID:3666] [RANK:0] total_num_steps: 1001[39m
[2025-06-20 21:48:11,244] [DEBUG] [axolotl.train.setup_model_and_tokenizer:58] [PID:3666] [RANK:0] loading tokenizer... mistralai/Mistral-7B-v0.1[39m
[2025-06-20 21:48:12,354] [DEBUG] [axolotl.load_tokenizer:401] [PID:3666] [RANK:0] EOS: 2 / </s>[39m
[2025-06-20 21:48:12,355] [DEBUG] [axolotl.load_tokenizer:402] [PID:3666] [RANK:0] BOS: 1 / <s>[39m
[2025-06-20 21:48:12,355] [DEBUG] [axolotl.load_tokenizer:403] [PID:3666] [RANK:0] PAD: 2 / </s>[39m
[2025-06-20 21:48:12,355] [DEBUG] [axolotl.load_tokenizer:404] [PID:3666] [RANK:0] UNK: 0 / <unk>[39m
[2025-06-20 21:48:12,355] [DEBUG] [axolotl.train.setup_model_and_tokenizer:73] [PID:3666] [RANK:0] loading model and peft_config...[39m
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now default to True since model is quantized.
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards:  50%|█████     | 1/2 [00:37<00:37, 37.08s/it]Downloading shards: 100%|██████████| 2/2 [00:54<00:00, 25.72s/it]Downloading shards: 100%|██████████| 2/2 [00:54<00:00, 27.43s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.85s/it]
[2025-06-20 21:49:12,485] [INFO] [axolotl.load_model:1191] [PID:3666] [RANK:0] cuda memory usage after model load: 4.157GB (+0.064GB cache, +0.555GB misc)[39m
[2025-06-20 21:49:12,520] [INFO] [axolotl.prepare_model:1097] [PID:3666] [RANK:0] converting PEFT model w/ prepare_model_for_kbit_training[39m
[2025-06-20 21:49:12,524] [INFO] [axolotl.load_model:1224] [PID:3666] [RANK:0] Converting modules to torch.bfloat16[39m
trainable params: 3,407,872 || all params: 7,245,139,968 || trainable%: 0.0470
[2025-06-20 21:49:12,609] [INFO] [axolotl.load_model:1285] [PID:3666] [RANK:0] cuda memory usage after adapters: 4.170GB (+1.059GB cache, +0.555GB misc)[39m
/workspace/axolotl/src/axolotl/core/trainers/base.py:379: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `AxolotlTrainer.__init__`. Use `processing_class` instead.
  super().__init__(*_args, **kwargs)
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
[2025-06-20 21:49:15,631] [INFO] [axolotl.train.save_initial_configs:337] [PID:3666] [RANK:0] Pre-saving adapter config to ./cha-out...[39m
[2025-06-20 21:49:15,631] [INFO] [axolotl.train.save_initial_configs:341] [PID:3666] [RANK:0] Pre-saving tokenizer to ./cha-out...[39m
[2025-06-20 21:49:15,638] [INFO] [axolotl.train.save_initial_configs:344] [PID:3666] [RANK:0] Pre-saving model config to ./cha-out...[39m
[2025-06-20 21:49:15,639] [INFO] [axolotl.train.execute_training:179] [PID:3666] [RANK:0] Starting trainer...[39m
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: ERROR api_key not configured (no-tty). call wandb.login(key=[your_api_key])
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/axolotl/src/axolotl/cli/train.py", line 116, in <module>
    fire.Fire(do_cli)
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/axolotl/src/axolotl/cli/train.py", line 90, in do_cli
    return do_train(parsed_cfg, parsed_cli_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/axolotl/src/axolotl/cli/train.py", line 46, in do_train
    model, tokenizer, trainer = train(cfg=cfg, dataset_meta=dataset_meta)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/axolotl/src/axolotl/train.py", line 507, in train
    execute_training(cfg, trainer, resume_from_checkpoint)
  File "/workspace/axolotl/src/axolotl/train.py", line 192, in execute_training
    trainer.train(resume_from_checkpoint=resume_from_checkpoint)
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/transformers/trainer.py", line 2232, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/transformers/trainer.py", line 2455, in _inner_training_loop
    self.control = self.callback_handler.on_train_begin(args, self.state, self.control)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/transformers/trainer_callback.py", line 507, in on_train_begin
    return self.call_event("on_train_begin", args, state, control)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/transformers/trainer_callback.py", line 557, in call_event
    result = getattr(callback, event)(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/transformers/integrations/integration_utils.py", line 916, in on_train_begin
    self.setup(args, state, model, **kwargs)
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/transformers/integrations/integration_utils.py", line 843, in setup
    self._wandb.init(
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/wandb/sdk/wandb_init.py", line 1482, in init
    wandb._sentry.reraise(e)
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/wandb/analytics/sentry.py", line 156, in reraise
    raise exc.with_traceback(sys.exc_info()[2])
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/wandb/sdk/wandb_init.py", line 1427, in init
    wi.maybe_login(init_settings)
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/wandb/sdk/wandb_init.py", line 175, in maybe_login
    wandb_login._login(
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/wandb/sdk/wandb_login.py", line 305, in _login
    key, key_status = wlogin.prompt_api_key()
                      ^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/wandb/sdk/wandb_login.py", line 234, in prompt_api_key
    raise UsageError("api_key not configured (no-tty). call " + directive)
wandb.errors.errors.UsageError: api_key not configured (no-tty). call wandb.login(key=[your_api_key])
[2025-06-20 21:49:17,099] [INFO] [wandb.sdk.mailbox.mailbox.close:129] [PID:3666] Closing mailbox, abandoning 0 handles.
Traceback (most recent call last):
  File "/root/miniconda3/envs/py3.11/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1172, in launch_command
    simple_launcher(args)
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/accelerate/commands/launch.py", line 762, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/root/miniconda3/envs/py3.11/bin/python', '-m', 'axolotl.cli.train', 'configs/config.yml', '--debug-num-examples', '0']' returned non-zero exit status 1.
[2025-06-20 21:49:19,323] [ERROR] [root.train:171] [PID:3411] Failed to train/fine-tune config 'configs/config.yml': Command '['accelerate', 'launch', '-m', 'axolotl.cli.train', 'configs/config.yml', '--debug-num-examples', '0']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/root/miniconda3/envs/py3.11/bin/axolotl", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/workspace/axolotl/src/axolotl/cli/main.py", line 322, in main
    cli()
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/click/core.py", line 1161, in __call__
    return self.main(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/click/core.py", line 1082, in main
    rv = self.invoke(ctx)
         ^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/click/core.py", line 1697, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/click/core.py", line 1443, in invoke
    return ctx.invoke(self.callback, **ctx.params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/click/core.py", line 788, in invoke
    return __callback(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/axolotl/src/axolotl/cli/utils.py", line 62, in wrapper
    return func(*args, **filtered_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/axolotl/src/axolotl/cli/main.py", line 173, in train
    raise exc
  File "/workspace/axolotl/src/axolotl/cli/main.py", line 158, in train
    subprocess.run(cmd, check=True)  # nosec B603
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/py3.11/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['accelerate', 'launch', '-m', 'axolotl.cli.train', 'configs/config.yml', '--debug-num-examples', '0']' returned non-zero exit status 1.
