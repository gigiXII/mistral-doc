The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
[2025-06-21 11:05:46,989] [INFO] [datasets.<module>:54] [PID:1317] PyTorch version 2.5.1+cu124 available.
[2025-06-21 11:05:48,122] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
df: /root/.triton/autotune: No such file or directory
[2025-06-21 11:05:48,414] [INFO] [root.spawn:60] [PID:1317] gcc -pthread -B /root/miniconda3/envs/py3.11/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /root/miniconda3/envs/py3.11/include -fPIC -O2 -isystem /root/miniconda3/envs/py3.11/include -fPIC -c /tmp/tmpjfjajzn0/test.c -o /tmp/tmpjfjajzn0/test.o
[2025-06-21 11:05:48,450] [INFO] [root.spawn:60] [PID:1317] gcc -pthread -B /root/miniconda3/envs/py3.11/compiler_compat /tmp/tmpjfjajzn0/test.o -laio -o /tmp/tmpjfjajzn0/a.out
[2025-06-21 11:05:48,942] [INFO] [root.spawn:60] [PID:1317] gcc -pthread -B /root/miniconda3/envs/py3.11/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /root/miniconda3/envs/py3.11/include -fPIC -O2 -isystem /root/miniconda3/envs/py3.11/include -fPIC -c /tmp/tmpd0wnl807/test.c -o /tmp/tmpd0wnl807/test.o
[2025-06-21 11:05:48,975] [INFO] [root.spawn:60] [PID:1317] gcc -pthread -B /root/miniconda3/envs/py3.11/compiler_compat /tmp/tmpd0wnl807/test.o -L/usr/local/cuda -L/usr/local/cuda/lib64 -lcufile -o /tmp/tmpd0wnl807/a.out
/workspace/axolotl/src/axolotl/monkeypatch/relora.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import ZeroRedundancyOptimizer
[33m[2025-06-21 11:05:49,735] [WARNING] [axolotl.utils.config.models.input.hint_lora_8bit:1425] [PID:1317] [RANK:0] We recommend setting `load_in_8bit: true` for LORA finetuning[39m
[2025-06-21 11:05:50,227] [INFO] [axolotl.normalize_config:236] [PID:1317] [RANK:0] cuda memory usage baseline: 0.000GB (+0.272GB misc)[39m

     #@@ #@@      @@# @@#
    @@  @@          @@  @@           =@@#                               @@                 #@    =@@#.
    @@    #@@@@@@@@@    @@           #@#@=                              @@                 #@     .=@@
      #@@@@@@@@@@@@@@@@@            =@# @#     ##=     ##    =####=+    @@      =#####+  =#@@###.   @@
    @@@@@@@@@@/  +@@/  +@@          #@  =@=     #@=   @@   =@#+  +#@#   @@    =@#+  +#@#   #@.      @@
    @@@@@@@@@@  ##@@  ##@@         =@#   @#      =@# @#    @@      @@   @@    @@      #@   #@       @@
     @@@@@@@@@@@@@@@@@@@@          #@=+++#@=      =@@#     @@      @@   @@    @@      #@   #@       @@
                                  =@#=====@@     =@# @#    @@      @@   @@    @@      #@   #@       @@
    @@@@@@@@@@@@@@@@  @@@@        #@      #@=   #@=  +@@   #@#    =@#   @@.   =@#    =@#   #@.      @@
                                 =@#       @#  #@=     #@   =#@@@@#=    +#@@=  +#@@@@#=    .##@@+   @@
    @@@@  @@@@@@@@@@@@@@@@

[2025-06-21 11:05:53,307] [DEBUG] [axolotl.load_tokenizer:401] [PID:1317] [RANK:0] EOS: 2 / </s>[39m
[2025-06-21 11:05:53,307] [DEBUG] [axolotl.load_tokenizer:402] [PID:1317] [RANK:0] BOS: 1 / <s>[39m
[2025-06-21 11:05:53,307] [DEBUG] [axolotl.load_tokenizer:403] [PID:1317] [RANK:0] PAD: 2 / </s>[39m
[2025-06-21 11:05:53,307] [DEBUG] [axolotl.load_tokenizer:404] [PID:1317] [RANK:0] UNK: 0 / <unk>[39m
[2025-06-21 11:05:53,308] [INFO] [axolotl.utils.data.sft.load_tokenized_prepared_datasets:252] [PID:1317] [RANK:0] Unable to find prepared dataset in last_run_prepared/c0dc037b6c74c09d179d6681f607b1e5[39m
[2025-06-21 11:05:53,308] [INFO] [axolotl.utils.data.sft.load_tokenized_prepared_datasets:253] [PID:1317] [RANK:0] Loading raw datasets...[39m
[33m[2025-06-21 11:05:53,308] [WARNING] [axolotl.utils.data.sft.load_tokenized_prepared_datasets:255] [PID:1317] [RANK:0] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset.[39m
[2025-06-21 11:05:53,308] [INFO] [axolotl.utils.data.sft.load_tokenized_prepared_datasets:262] [PID:1317] [RANK:0] No seed provided, using default seed of 42[39m
Generating train split:   0%|          | 0/179 [00:00<?, ? examples/s]Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 179/179 [00:00<00:00, 1480.20 examples/s]Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 179/179 [00:00<00:00, 903.34 examples/s] 
Generating test split:   0%|          | 0/20 [00:00<?, ? examples/s]Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 658.47 examples/s]
[2025-06-21 11:05:59,589] [INFO] [axolotl.utils.data.sft.get_dataset_wrapper:457] [PID:1317] [RANK:0] Loading dataset with base_type: completion and prompt_style: None[39m
Tokenizing Prompts (num_proc=32):   0%|          | 0/179 [00:00<?, ? examples/s]Tokenizing Prompts (num_proc=32):   3%|â–Ž         | 6/179 [00:00<00:04, 36.22 examples/s]Tokenizing Prompts (num_proc=32):  10%|â–ˆ         | 18/179 [00:00<00:02, 63.01 examples/s]Tokenizing Prompts (num_proc=32):  16%|â–ˆâ–Œ        | 29/179 [00:00<00:01, 79.23 examples/s]Tokenizing Prompts (num_proc=32):  23%|â–ˆâ–ˆâ–Ž       | 41/179 [00:00<00:01, 73.90 examples/s]Tokenizing Prompts (num_proc=32):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 68/179 [00:00<00:00, 118.71 examples/s]Tokenizing Prompts (num_proc=32):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 90/179 [00:00<00:00, 110.66 examples/s]Tokenizing Prompts (num_proc=32):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 108/179 [00:01<00:00, 121.88 examples/s]Tokenizing Prompts (num_proc=32):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 135/179 [00:01<00:00, 152.37 examples/s]Tokenizing Prompts (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 157/179 [00:01<00:00, 102.35 examples/s]Tokenizing Prompts (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 174/179 [00:01<00:00, 95.27 examples/s] Tokenizing Prompts (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 179/179 [00:02<00:00, 86.95 examples/s]
[2025-06-21 11:06:02,639] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:177] [PID:1317] [RANK:0] min_input_len: 6[39m
[2025-06-21 11:06:02,639] [INFO] [axolotl.utils.data.utils.drop_long_seq_in_dataset:179] [PID:1317] [RANK:0] max_input_len: 2048[39m
Dropping Long Sequences (num_proc=32):   0%|          | 0/2107 [00:00<?, ? examples/s]Dropping Long Sequences (num_proc=32):   3%|â–Ž         | 66/2107 [00:00<00:05, 374.80 examples/s]Dropping Long Sequences (num_proc=32):   6%|â–‹         | 132/2107 [00:00<00:03, 498.86 examples/s]Dropping Long Sequences (num_proc=32):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 923/2107 [00:00<00:00, 3339.40 examples/s]Dropping Long Sequences (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2107/2107 [00:00<00:00, 5800.14 examples/s]Dropping Long Sequences (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2107/2107 [00:00<00:00, 3408.84 examples/s]
[2025-06-21 11:06:04,213] [INFO] [axolotl.utils.data.sft.load_tokenized_prepared_datasets:332] [PID:1317] [RANK:0] Saving merged prepared dataset to disk... last_run_prepared/c0dc037b6c74c09d179d6681f607b1e5[39m
Saving the dataset (0/1 shards):   0%|          | 0/2107 [00:00<?, ? examples/s]Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2107/2107 [00:00<00:00, 30066.03 examples/s]Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2107/2107 [00:00<00:00, 29955.15 examples/s]
[2025-06-21 11:06:04,533] [DEBUG] [axolotl.calculate_total_num_steps:403] [PID:1317] [RANK:0] total_num_tokens: 3_917_447[39m
[2025-06-21 11:06:04,577] [DEBUG] [axolotl.calculate_total_num_steps:421] [PID:1317] [RANK:0] `total_supervised_tokens: 4_127_813`[39m
[2025-06-21 11:06:04,577] [DEBUG] [axolotl.calculate_total_num_steps:499] [PID:1317] [RANK:0] total_num_steps: 1001[39m
[2025-06-21 11:06:04,581] [DEBUG] [axolotl.train.setup_model_and_tokenizer:58] [PID:1317] [RANK:0] loading tokenizer... mistralai/Mistral-7B-v0.1[39m
[2025-06-21 11:06:05,695] [DEBUG] [axolotl.load_tokenizer:401] [PID:1317] [RANK:0] EOS: 2 / </s>[39m
[2025-06-21 11:06:05,695] [DEBUG] [axolotl.load_tokenizer:402] [PID:1317] [RANK:0] BOS: 1 / <s>[39m
[2025-06-21 11:06:05,695] [DEBUG] [axolotl.load_tokenizer:403] [PID:1317] [RANK:0] PAD: 2 / </s>[39m
[2025-06-21 11:06:05,695] [DEBUG] [axolotl.load_tokenizer:404] [PID:1317] [RANK:0] UNK: 0 / <unk>[39m
[2025-06-21 11:06:05,695] [DEBUG] [axolotl.train.setup_model_and_tokenizer:73] [PID:1317] [RANK:0] loading model and peft_config...[39m
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now default to True since model is quantized.
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:37<00:37, 37.80s/it]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:55<00:00, 25.72s/it]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:55<00:00, 27.53s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:02<00:02,  2.40s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.93s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  2.00s/it]
[2025-06-21 11:07:06,301] [INFO] [axolotl.load_model:1191] [PID:1317] [RANK:0] cuda memory usage after model load: 4.157GB (+0.064GB cache, +0.454GB misc)[39m
[2025-06-21 11:07:06,340] [INFO] [axolotl.prepare_model:1097] [PID:1317] [RANK:0] converting PEFT model w/ prepare_model_for_kbit_training[39m
[2025-06-21 11:07:06,342] [INFO] [axolotl.load_model:1224] [PID:1317] [RANK:0] Converting modules to torch.bfloat16[39m
trainable params: 3,407,872 || all params: 7,245,139,968 || trainable%: 0.0470
[2025-06-21 11:07:06,429] [INFO] [axolotl.load_model:1285] [PID:1317] [RANK:0] cuda memory usage after adapters: 4.170GB (+1.059GB cache, +0.454GB misc)[39m
/workspace/axolotl/src/axolotl/core/trainers/base.py:379: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `AxolotlTrainer.__init__`. Use `processing_class` instead.
  super().__init__(*_args, **kwargs)
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
[2025-06-21 11:07:07,283] [INFO] [axolotl.train.save_initial_configs:337] [PID:1317] [RANK:0] Pre-saving adapter config to ./cha-out...[39m
[2025-06-21 11:07:07,283] [INFO] [axolotl.train.save_initial_configs:341] [PID:1317] [RANK:0] Pre-saving tokenizer to ./cha-out...[39m
[2025-06-21 11:07:07,292] [INFO] [axolotl.train.save_initial_configs:344] [PID:1317] [RANK:0] Pre-saving model config to ./cha-out...[39m
[2025-06-21 11:07:07,293] [INFO] [axolotl.train.execute_training:179] [PID:1317] [RANK:0] Starting trainer...[39m
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: ERROR api_key not configured (no-tty). call wandb.login(key=[your_api_key])
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/axolotl/src/axolotl/cli/train.py", line 116, in <module>
    fire.Fire(do_cli)
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/axolotl/src/axolotl/cli/train.py", line 90, in do_cli
    return do_train(parsed_cfg, parsed_cli_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/axolotl/src/axolotl/cli/train.py", line 46, in do_train
    model, tokenizer, trainer = train(cfg=cfg, dataset_meta=dataset_meta)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/axolotl/src/axolotl/train.py", line 507, in train
    execute_training(cfg, trainer, resume_from_checkpoint)
  File "/workspace/axolotl/src/axolotl/train.py", line 192, in execute_training
    trainer.train(resume_from_checkpoint=resume_from_checkpoint)
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/transformers/trainer.py", line 2232, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/transformers/trainer.py", line 2455, in _inner_training_loop
    self.control = self.callback_handler.on_train_begin(args, self.state, self.control)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/transformers/trainer_callback.py", line 507, in on_train_begin
    return self.call_event("on_train_begin", args, state, control)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/transformers/trainer_callback.py", line 557, in call_event
    result = getattr(callback, event)(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/transformers/integrations/integration_utils.py", line 916, in on_train_begin
    self.setup(args, state, model, **kwargs)
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/transformers/integrations/integration_utils.py", line 843, in setup
    self._wandb.init(
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/wandb/sdk/wandb_init.py", line 1482, in init
    wandb._sentry.reraise(e)
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/wandb/analytics/sentry.py", line 156, in reraise
    raise exc.with_traceback(sys.exc_info()[2])
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/wandb/sdk/wandb_init.py", line 1427, in init
    wi.maybe_login(init_settings)
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/wandb/sdk/wandb_init.py", line 175, in maybe_login
    wandb_login._login(
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/wandb/sdk/wandb_login.py", line 305, in _login
    key, key_status = wlogin.prompt_api_key()
                      ^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/wandb/sdk/wandb_login.py", line 234, in prompt_api_key
    raise UsageError("api_key not configured (no-tty). call " + directive)
wandb.errors.errors.UsageError: api_key not configured (no-tty). call wandb.login(key=[your_api_key])
[2025-06-21 11:07:08,804] [INFO] [wandb.sdk.mailbox.mailbox.close:129] [PID:1317] Closing mailbox, abandoning 0 handles.
Traceback (most recent call last):
  File "/root/miniconda3/envs/py3.11/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1172, in launch_command
    simple_launcher(args)
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/accelerate/commands/launch.py", line 762, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/root/miniconda3/envs/py3.11/bin/python', '-m', 'axolotl.cli.train', 'configs/config.yml', '--debug-num-examples', '0']' returned non-zero exit status 1.
[2025-06-21 11:07:11,002] [ERROR] [root.train:171] [PID:1062] Failed to train/fine-tune config 'configs/config.yml': Command '['accelerate', 'launch', '-m', 'axolotl.cli.train', 'configs/config.yml', '--debug-num-examples', '0']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/root/miniconda3/envs/py3.11/bin/axolotl", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/workspace/axolotl/src/axolotl/cli/main.py", line 322, in main
    cli()
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/click/core.py", line 1161, in __call__
    return self.main(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/click/core.py", line 1082, in main
    rv = self.invoke(ctx)
         ^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/click/core.py", line 1697, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/click/core.py", line 1443, in invoke
    return ctx.invoke(self.callback, **ctx.params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/click/core.py", line 788, in invoke
    return __callback(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/axolotl/src/axolotl/cli/utils.py", line 62, in wrapper
    return func(*args, **filtered_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/axolotl/src/axolotl/cli/main.py", line 173, in train
    raise exc
  File "/workspace/axolotl/src/axolotl/cli/main.py", line 158, in train
    subprocess.run(cmd, check=True)  # nosec B603
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/py3.11/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['accelerate', 'launch', '-m', 'axolotl.cli.train', 'configs/config.yml', '--debug-num-examples', '0']' returned non-zero exit status 1.
